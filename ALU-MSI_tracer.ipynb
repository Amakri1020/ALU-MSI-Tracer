{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import twobitreader\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "import mmap\n",
    "from tqdm import tqdm_notebook\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(seq : str) -> str:\n",
    "    \"\"\"A simple function for generating the opposite strand in a DNA sequence.\n",
    "    \n",
    "    Takes a string as input, e.g. 'ACTG', and outputs the reverse strand, 'CAGT'.\"\"\"\n",
    "    rev = \"\"\n",
    "    for base in reversed(seq):\n",
    "        if base.upper() == 'T':\n",
    "            rev += 'A'\n",
    "        elif base.upper() == 'A':\n",
    "            rev += 'T'\n",
    "        elif base.upper() == 'G':\n",
    "            rev += 'C'\n",
    "        elif base.upper() == 'C':\n",
    "            rev += 'G'\n",
    "        else:\n",
    "            #print(\"Invalid Sequence\")\n",
    "            break\n",
    "    return rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_polyA_pure(seq : str, min_length=10) -> str:\n",
    "    \"\"\"Extracts a pure polyA of at least min_length bases following the 17-base Alu primer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : str\n",
    "        The DNA sequence beginning with the 17-base Alu tail primer\n",
    "    min_length : int, optional\n",
    "        Minimum length required to consider the pure polyA valid, default is 10\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    polyA\n",
    "        The pure polyA, unless one is not found, in which case an empty string is returned\n",
    "    \"\"\"\n",
    "    \n",
    "    countA = 0\n",
    "    polyA = \"\"\n",
    "    \n",
    "    for i in range(17, len(seq)):\n",
    "        if seq[i].upper() == 'A':\n",
    "            countA += 1\n",
    "            polyA += seq[i]\n",
    "        elif seq[i].upper() != 'A' and countA >= min_length:\n",
    "            return polyA\n",
    "        else:\n",
    "            polyA = \"\"\n",
    "            return polyA\n",
    "        \n",
    "    return polyA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_pure_primer(file : str) -> list:\n",
    "    relevant = []\n",
    "    with open(file) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[0][0] != 'M':\n",
    "                print('Whoops')\n",
    "                pass\n",
    "            else:\n",
    "                if(row[9][:17] == 'GAGCGAGACTCCGTCTC'):\n",
    "                    polyA = extract_polyA_pure(row[9], 10)\n",
    "                    if not polyA == \"\":\n",
    "                        relevant.append([row[2], row[3], polyA, len(polyA), row[9]])\n",
    "                if(row[9][-17:] == 'GAGACGGAGTCTCGCTC'):\n",
    "                    polyA = extract_polyA_pure(reverse(row[9]), 10)\n",
    "                    if not polyA == \"\":\n",
    "                        relevant.append([row[2], row[3], polyA, len(polyA), row[9]])\n",
    "    return relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function so TQDM can show progress, possibly not worth for large files, slows things down\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n",
    "\n",
    "def info_single_file(fname : str, outdir: str, required_occurrences : int):\n",
    "    \"\"\"Generates an info file for the given .CSV file, containing all reads occurring more than 'required_occurrences' times.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Filename of the .csv file for which the info file is being generated. This file must be in\n",
    "        the format of a converted SAM file, e.g. by exporting a SAM file to CSV in Excel,\n",
    "        or by using the sam_to_csv function provided in this file.\n",
    "    outdir : str\n",
    "        Directory for info files to be written into\n",
    "    required_occurrences : int\n",
    "        Minimum number of reads from the same genomic position required to qualify a position\n",
    "        for inclusion in the output info file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        A .csv file will be written to the current directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    group = []\n",
    "    last = 0\n",
    "    head = 'TGGTCTCGATCTCCTGACCTC'\n",
    "    head_r = 'GAGGTCAGGAGATCGAGACCA'\n",
    "    tail = 'GAGCGAGACTCCGTCTCA'\n",
    "    tail_r = 'TGAGACGGAGTCTCGCTC'\n",
    "    \n",
    "    # Get first position number for group identification\n",
    "    with open(fname) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            if row[0][0] != 'M':\n",
    "                pass\n",
    "            else:\n",
    "                last = row[3]\n",
    "                break\n",
    "        \n",
    "    # Main loop\n",
    "    with open(fname) as csvfile, open('temp.csv', 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        \n",
    "        # Initialize all info variables\n",
    "        total_reads = 0\n",
    "        primer_polyAs = 0\n",
    "        total_groups = 0\n",
    "        #unique_groups_primer = 0\n",
    "        thresh_groups = 0\n",
    "        primers = 0\n",
    "        th = 0\n",
    "        hh = 0\n",
    "        tt = 0\n",
    "        h = 0\n",
    "        t = 0\n",
    "        \n",
    "        \n",
    "        for row in tqdm(reader, total=get_num_lines(fname)):\n",
    "            total_reads += 1\n",
    "            if row[0][0] != 'M':\n",
    "                pass\n",
    "            else:\n",
    "                # Individual primer occurrence counts in full sequence\n",
    "                seq = row[9]\n",
    "                if head in seq or head_r in seq:\n",
    "                    h += 1\n",
    "                if tail in seq or tail_r in seq:\n",
    "                    t += 1\n",
    "                if tail in seq and head_r in seq:\n",
    "                    th += 1\n",
    "                elif tail in seq and tail_r in seq:\n",
    "                    tt += 1\n",
    "                elif head in seq and head_r in seq:\n",
    "                    hh += 1\n",
    "                \n",
    "                current = row[3]\n",
    "                \n",
    "                # On position change, write current position group to output if we have enough occurrences\n",
    "                if current != last:\n",
    "                    total_groups += 1\n",
    "                    if len(group) >= required_occurrences:\n",
    "                        thresh_groups += 1\n",
    "                        for entry in group:\n",
    "                            wr.writerow(entry)\n",
    "                    group = []\n",
    "                \n",
    "                # Get pure polyA if we find primer at beginning or end of sequence \n",
    "                if(row[9][:17] == 'GAGCGAGACTCCGTCTC'):\n",
    "                    primers += 1\n",
    "                    polyA = extract_polyA_pure(row[9], 10)\n",
    "                    if not polyA == \"\":\n",
    "                        primer_polyAs += 1\n",
    "                        group.append([row[2], row[3], polyA, len(polyA), row[9]])\n",
    "                        \n",
    "                # Reverse sequence if primer is at the end so all seq's are in terms of top strand\n",
    "                elif(row[9][-17:] == 'GAGACGGAGTCTCGCTC'):\n",
    "                    primers += 1\n",
    "                    polyA = extract_polyA_pure(reverse(row[9]), 10)\n",
    "                    if not polyA == \"\":\n",
    "                        primer_polyAs += 1\n",
    "                        group.append([row[2], row[3], polyA, len(polyA), row[9]])\n",
    "                last = current\n",
    "                    \n",
    "                        \n",
    "    split = fname.split('/')\n",
    "    outname = outdir + split[-1][:-4] +'_info'+str(required_occurrences)+'.csv'\n",
    "        \n",
    "    with open(outname, 'w', newline='') as myfile, open('temp.csv') as csvfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(['Tail-Head: '+str(th)+ ' -- Tail-Tail: ' + str(tt)+' -- Head-Head: '+str(hh)\n",
    "                     + ' -- All Head Occurrences: '+str(h)+ ' -- All Tail Occurrences: '+str(t)])\n",
    "        wr.writerow(['Total Reads: ' + str(total_reads) + ' -- Total Groups: ' + str(total_groups) \n",
    "                     + ' -- Normalized(100k): ' + str(int((100000 * (float(total_groups) / float(total_reads)))))\n",
    "                     + ' -- Groups with over ' + str(required_occurrences) + ' occurrences: ' + str(thresh_groups)])\n",
    "        wr.writerow(['All primers: ' + str(primers) \n",
    "                     + '-- Pure polyAs next to tail primer: ' + str(primer_polyAs)]) #+ ' -- Total tail-polyA groups: ' + str(unique_groups_primer)])\n",
    "        wr.writerow([])\n",
    "        wr.writerow(['Chromosome', 'Position', 'PolyA', 'Length', 'Original Sequence'])\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            wr.writerow(row)\n",
    "    os.remove('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_reads(fname, fname_tumor, outname1, outname2, verbose=False):\n",
    "    \"\"\"Compares two info files and outputs a .csv file for each, containing only shared reads between the files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        Filename of the first info file, typically the one that does not contain any tumor sample although this is\n",
    "        not required.\n",
    "    fname_tumor : str\n",
    "        Filename of the second info file, typically the one containing some trace of the tumor.\n",
    "    outname1 : str\n",
    "        Name of the first output file containing the reads from fname that are shared with fname_tumor\n",
    "        Typical output name would be something like 'MMX_MMY_shared_reads.csv'\n",
    "    outname2 : str\n",
    "        Name of the second output file containing the reads from fname_tumor that are shared with fname\n",
    "        Typical output name would be something like 'MMY_MMX_shared_reads.csv'\n",
    "    verbose : boolean, optional\n",
    "        If true, some progress messages will be written to stdout while the function executes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Two .csv files will be written to the output paths specified in the function arguments.\n",
    "    \"\"\"\n",
    "    \n",
    "    IDs = set()\n",
    "    IDs_tumor = set()\n",
    "    \n",
    "    # Iterate through both info files and gather all IDs (format 'Chromosome#-Position') in a set\n",
    "    with open(fname) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i in range(5):\n",
    "            next(reader)\n",
    "        for row in reader:\n",
    "            IDs.add(str(row[0])+'-'+str(row[1]))\n",
    "            \n",
    "    if verbose: print(fname + ' IDs added.') \n",
    "\n",
    "    with open(fname_tumor) as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i in range(5):\n",
    "            next(reader)\n",
    "        for row in reader:\n",
    "            IDs_tumor.add(str(row[0])+'-'+str(row[1]))\n",
    "            \n",
    "    if verbose: print(fname_tumor + ' IDs added.') \n",
    "\n",
    "    # Set intersection gives the positions shared by both files\n",
    "    shared = IDs & IDs_tumor\n",
    "    count = len(shared)\n",
    "    \n",
    "    # Write shared reads files, for each only keeping rows where the position is in the shared set\n",
    "    with open(fname) as csvfile, open(outname1, 'w', newline='') as myfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i in range(5):\n",
    "            next(reader)\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(['Shared Groups: ' + str(count)])\n",
    "        wr.writerow([])\n",
    "        wr.writerow(['Chromosome', 'Position', 'PolyA', 'Length', 'Original Sequence'])\n",
    "        for row in reader:\n",
    "            key = str(row[0])+'-'+str(row[1])\n",
    "            if key in shared:\n",
    "                wr.writerow(row)\n",
    "                \n",
    "    if verbose: print(outname1 + ' written.')\n",
    "                \n",
    "    with open(fname_tumor) as csvfile, open(outname2, 'w', newline='') as myfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for i in range(5):\n",
    "            next(reader)\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(['Shared Groups: ' + str(count)])\n",
    "        wr.writerow([])\n",
    "        wr.writerow(['Chromosome', 'Position', 'PolyA', 'Length', 'Original Sequence'])\n",
    "        for row in reader:\n",
    "            key = str(row[0])+'-'+str(row[1])\n",
    "            if key in shared:\n",
    "                wr.writerow(row)\n",
    "                \n",
    "    if verbose: print(outname2 + ' written.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphs(file1 : str, file2 : str, outdir : str, r3 : bool, r6 : bool, X : float, verbose=False):\n",
    "    \"\"\"Generates comparison graphs for two input shared_reads files if the given conditions are met.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file1 : str\n",
    "        Name of the first shared_reads file.\n",
    "    fname_tumor : str\n",
    "        Name of the second shared_reads file.\n",
    "    outdir : str\n",
    "        Directory for all generated graphs to be written to (automatically created if it does not already exist).\n",
    "    r3 : bool\n",
    "        Rule 3: The two smallest polyA lengths must be from the tumor sample, and at least one must have\n",
    "        more than two occurrences. If set to True, this rule must be met for any graphs generated.\n",
    "    r6 : bool\n",
    "        Rule 6: X % of tumor reads occur before the first non-tumor read. If set to True, this rule must\n",
    "        be met for any graphs generated.\n",
    "    X : float\n",
    "        Percentage of reads required to occur before the first non-tumor read for rule 6. Range 0 to 1.\n",
    "    verbose : boolean, optional\n",
    "        If true, some progress messages will be written to stdout while the function executes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Graphs will be written in outdir for all samples meeting the input conditions.\n",
    "    \"\"\"\n",
    "    \n",
    "    lengths_normal = {}\n",
    "    lengths_tumor = {}\n",
    "    \n",
    "    if X < 0 or X > 1:\n",
    "        print(\"X must be a value between 0 and 1\")\n",
    "        return None\n",
    "    \n",
    "    ## Initialize keys with empty dicts for every position\n",
    "    ## Key format 'Chromosome#-Position'\n",
    "    with open(file1, 'r') as myfile:\n",
    "        cr = csv.reader(myfile)\n",
    "        # Skip header rows\n",
    "        for i in range(4):\n",
    "            next(cr)\n",
    "            \n",
    "        for row in cr:\n",
    "            key = str(row[0])+'-'+str(row[1])\n",
    "            if not key in lengths_normal:\n",
    "                lengths_normal[key] = [] \n",
    "                lengths_tumor[key] = []\n",
    "    \n",
    "    if verbose : print(\"Dict keys populated.\")\n",
    "        \n",
    "    ## For each file, store the lengths of all polyAs associated with every key\n",
    "    with open(file1,  'r') as myfile:\n",
    "        cr = csv.reader(myfile)\n",
    "        for i in range(4):\n",
    "            next(cr)\n",
    "            \n",
    "        for row in cr:\n",
    "            key = str(row[0]+'-'+str(row[1]))\n",
    "            lengths_normal[key].append(len(row[2]))\n",
    "    if verbose : print(\"Normal Lengths stored.\")\n",
    "    \n",
    "    with open(file2,  'r') as myfile:\n",
    "        cr = csv.reader(myfile)\n",
    "        for i in range(4):\n",
    "            next(cr)\n",
    "            \n",
    "        for row in cr:\n",
    "            key = str(row[0]+'-'+str(row[1]))\n",
    "            lengths_tumor[key].append(len(row[2]))\n",
    "    if verbose : print(\"Tumor Lengths Stored\")\n",
    "    \n",
    "    ## For each key, determine whether it qualifies to be graphed, and if so generate and save graphs in outdir.\n",
    "    for key, value in lengths_normal.items():\n",
    "        f1 = lengths_normal[key]\n",
    "        f2 = lengths_tumor[key]\n",
    "        # Range of lengths for this key\n",
    "        index = np.arange(min(min(f1),min(f2)), max(max(f1),max(f2)+1))\n",
    "        # Lists containing identically ordered counts of polyA lengths for each sample\n",
    "        f1count = []\n",
    "        f2count = []\n",
    "        for idx in index:\n",
    "            f1count.append(f1.count(idx))\n",
    "            f2count.append(f2.count(idx))\n",
    "\n",
    "        \n",
    "        two_lowest = False\n",
    "        cond1 = False\n",
    "        cond2 = False\n",
    "        \n",
    "        ## Rule 3: The two smallest polyA lengths must be from the tumor sample, and at least one must have\n",
    "        ## more than two occurrences.\n",
    "        for idx in index:\n",
    "            # If both conditions are met, Rule 3 is met and we set it to True and break\n",
    "            if(cond1 and cond2):\n",
    "                two_lowest = True\n",
    "                break\n",
    "                \n",
    "            # If we find a non-zero read from the non-tumor sample before meeting our conditions, Rule 3 is broken\n",
    "            if f1.count(idx) != 0:\n",
    "                break\n",
    "            elif f2.count(idx) >= 2: # One of our reads must have 2 or more occurrences (Cond2).\n",
    "                # If Cond2 is already met and we have a second read with 2+ occurrences, that also satisfies Cond1.\n",
    "                if cond2:\n",
    "                    cond1 = True\n",
    "                else:\n",
    "                    cond2 = True\n",
    "            elif f2.count(idx) >= 1:  # One of our reads needs only 1 or more occurrences (Cond1).\n",
    "                cond1 = True\n",
    "                \n",
    "        ## Rule 6: X % of tumor reads occur before the first non-tumor read.\n",
    "        bluecount = 0\n",
    "        for idx in index:\n",
    "            if f1.count(idx) == 0:\n",
    "                bluecount += f2.count(idx)\n",
    "            else:\n",
    "                break\n",
    "        if (float(float(bluecount) / float(sum(f2count)))) > X:\n",
    "            rule6 = True\n",
    "        else:\n",
    "            rule6 = False\n",
    "        \n",
    "        # Labels for the graphs to be created, ex. 'DSN/Shared_reads/MM12_MM5_shared_reads.csv' -> 'MM12'\n",
    "        # TODO redo this using path module and account for other possible file names.\n",
    "        split = file1.split('/')\n",
    "        label1 = split[-1][:4]\n",
    "        split = file2.split('/')\n",
    "        label2 = split[-1][:4]\n",
    "        \n",
    "        # Boolean formula, applies whichever rules were set to True in function input\n",
    "        condition = (not r3 or two_lowest) and (not r6 or rule6)\n",
    "        \n",
    "        if condition:\n",
    "            fig, ax = plt.subplots()\n",
    "            bar_width = 0.35\n",
    "            opacity = 0.9\n",
    "            ax.bar(index, f1count, bar_width, alpha=opacity, color='r', label=label1)\n",
    "            ax.bar(index+bar_width, f2count, bar_width, alpha=opacity, color='b', label=label2)\n",
    "            plt.xticks(np.arange(min(index), max(index)+1, 1.0))\n",
    "            ax.set_xlabel('Length of PolyA')\n",
    "            ax.set_ylabel('Reads')\n",
    "            split = key.split('-')\n",
    "            # Separate chromosome# and position for Graph title\n",
    "            title = (split[0], split[1])\n",
    "            ax.set_title('Chromosome ' + title[0] + ': ' + title[1])\n",
    "            ax.legend()\n",
    "            direc = outdir\n",
    "            if not os.path.exists(direc):\n",
    "                os.makedirs(direc)\n",
    "            #TODO : Fix this in the *extremely* unlikely case of duplicate sequence positions in separate genomes\n",
    "            plt.savefig(os.path.join(direc, entry[1]))\n",
    "            plt.close(fig)\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_graphs(dirs : list, outname : str):\n",
    "    \"\"\"Identifies shared graphs in a list of directories containing graphs, outputs results to a csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dirs : list\n",
    "        List of directories\n",
    "    fname_tumor : str\n",
    "        Name of the second shared_reads file.\n",
    "    outdir : str\n",
    "        Directory for all generated graphs to be written to (automatically created if it does not already exist).\n",
    "    r3 : bool\n",
    "        Rule 3: The two smallest polyA lengths must be from the tumor sample, and at least one must have\n",
    "        more than two occurrences. If set to True, this rule must be met for any graphs generated.\n",
    "    r6 : bool\n",
    "        Rule 6: X % of tumor reads occur before the first non-tumor read. If set to True, this rule must\n",
    "        be met for any graphs generated.\n",
    "    X : float\n",
    "        Percentage of reads required to occur before the first non-tumor read for rule 6. Range 0 to 1.\n",
    "    verbose : boolean, optional\n",
    "        If true, some progress messages will be written to stdout while the function executes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Graphs will be written in outdir for all samples meeting the input conditions.\n",
    "    \"\"\"\n",
    "    allgraphs = []\n",
    "    for direc in dirs:\n",
    "        onlyfiles = [f for f in listdir(direc) if isfile(join(direc, f))]\n",
    "        allgraphs.append(onlyfiles)\n",
    "    \n",
    "    res = list(set.intersection(*map(set, allgraphs)))\n",
    "    \n",
    "    with open(outname, 'w', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        for x in res:\n",
    "            wr.writerow([x])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 15222323/15222323 [04:43<00:00, 53765.85it/s]\n"
     ]
    }
   ],
   "source": [
    "info_single_file('HiSeq/Files/MM1_1d0-3.csv', 'HiSeq/Info/', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function info_single_file in module __main__:\n",
      "\n",
      "info_single_file(fname: str, outdir: str, required_occurrences: int)\n",
      "    Generates an info file for the given .CSV file, containing all reads occurring more than 'required_occurrences' times.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fname : str\n",
      "        Filename of the .csv file for which the info file is being generated. This file must be in\n",
      "        the format of a converted SAM file, e.g. by exporting a SAM file to CSV in Excel,\n",
      "        or by using the sam_to_csv function provided in this file.\n",
      "    outdir : str\n",
      "        Directory for info files to be written into\n",
      "    required_occurrences : int\n",
      "        Minimum number of reads from the same genomic position required to qualify a position\n",
      "        for inclusion in the output info file.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    None\n",
      "        A .csv file will be written to the current directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(info_single_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 15223794/15223794 [04:26<00:00, 57058.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 15224796/15224796 [04:21<00:00, 58327.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 15226601/15226601 [04:21<00:00, 58206.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 15043661/15043661 [04:22<00:00, 57215.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 15539520/15539520 [04:21<00:00, 59327.05it/s]\n"
     ]
    }
   ],
   "source": [
    "info_single_file('HiSeq/Files/MM1_2d0-3.csv', 'HiSeq/Info/', 20)\n",
    "info_single_file('HiSeq/Files/MM1_3d0-3.csv', 'HiSeq/Info/', 20)\n",
    "info_single_file('HiSeq/Files/MM1_4d0-3.csv', 'HiSeq/Info/', 20)\n",
    "\n",
    "info_single_file('HiSeq/Files/MM2_d0-3.csv', 'HiSeq/Info/', 20)\n",
    "info_single_file('HiSeq/Files/MM3_d0-3.csv', 'HiSeq/Info/', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
